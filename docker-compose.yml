# Docker Compose configuration for RRRalgorithms

# ============================================================================
# RRRalgorithms - Optimized Docker Compose Configuration
# ============================================================================
# Consolidated and optimized for better resource utilization
# - Reduced from 11 services to 9 services
# - Optimized network topology (3 networks instead of 6)
# - Improved health checks and resource limits
# - Database connection pooling enabled
# ============================================================================

services:
  # ==========================================================================
  # Core Services
  # ==========================================================================

  neural-network:
    build:
      context: ./src/services/neural-network
      dockerfile: Dockerfile
    container_name: rrr-neural-network
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ./src/services/neural-network/models:/app/models
      - ./src/services/neural-network/logs:/app/logs
      - ./src/services/neural-network/data:/app/data
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - LOG_LEVEL=INFO
      - CUDA_VISIBLE_DEVICES=0
      - DATABASE_POOL_MIN=5
      - DATABASE_POOL_MAX=15
      - MODEL_CACHE_SIZE=10
      - MODEL_CACHE_TTL=3600
    networks:
      - rrr-backend
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; import transformers; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  data-pipeline:
    build:
      context: ./src/services/data-pipeline
      dockerfile: Dockerfile
    container_name: rrr-data-pipeline
    restart: unless-stopped
    ports:
      - "8001:8001"
    volumes:
      - ./src/services/data-pipeline/logs:/app/logs
      - ./src/services/data-pipeline/data:/app/data
      - ./src/services/data-pipeline/cache:/app/cache
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - POLYGON_API_KEY=${POLYGON_API_KEY}
      - PERPLEXITY_API_KEY=${PERPLEXITY_API_KEY}
      - LOG_LEVEL=INFO
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_POOL_MIN=10
      - DATABASE_POOL_MAX=30
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - rrr-backend
    healthcheck:
      test: ["CMD", "python", "-c", "from data_pipeline.supabase.client import SupabaseClient; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  trading-engine:
    build:
      context: ./src/services/trading-engine
      dockerfile: Dockerfile
    container_name: rrr-trading-engine
    restart: unless-stopped
    ports:
      - "8002:8002"
      - "8005:8005"  # API integration endpoints
    volumes:
      - ./src/services/trading-engine/logs:/app/logs
      - ./src/services/trading-engine/orders:/app/orders
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - COINBASE_API_KEY=${COINBASE_API_KEY}
      - COINBASE_API_SECRET=${COINBASE_API_SECRET}
      - POLYGON_API_KEY=${POLYGON_API_KEY}
      - PAPER_TRADING=true
      - LIVE_TRADING=false
      - LOG_LEVEL=INFO
      - DATABASE_POOL_MIN=10
      - DATABASE_POOL_MAX=20
      - ENABLE_AI_VALIDATION=true
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - rrr-backend
    healthcheck:
      test: ["CMD", "python", "-c", "from engine.oms.order_manager import OrderManager; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  risk-management:
    build:
      context: ./src/services/risk-management
      dockerfile: Dockerfile
    container_name: rrr-risk-management
    restart: unless-stopped
    ports:
      - "8003:8003"
    volumes:
      - ./src/services/risk-management/logs:/app/logs
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - MAX_DAILY_LOSS_PCT=0.03
      - MAX_POSITION_SIZE_PCT=0.10
      - LOG_LEVEL=INFO
    networks:
      - rrr-backend
    healthcheck:
      test: ["CMD", "python", "-c", "from risk.monitors.portfolio_risk import PortfolioRiskMonitor; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  backtesting:
    build:
      context: ./src/services/backtesting
      dockerfile: Dockerfile
    container_name: rrr-backtesting
    restart: unless-stopped
    ports:
      - "8004:8004"
    volumes:
      - ./src/services/backtesting/logs:/app/logs
      - ./src/services/backtesting/results:/app/results
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - LOG_LEVEL=INFO
      - DATABASE_POOL_MIN=5
      - DATABASE_POOL_MAX=15
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - rrr-backend
    healthcheck:
      test: ["CMD", "python", "-c", "from backtest.engine.backtest_engine import BacktestEngine; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # api-integration merged into trading-engine service
  # API endpoints now available on trading-engine:8005

  quantum-optimization:
    build:
      context: ./src/services/quantum-optimization
      dockerfile: Dockerfile
    container_name: rrr-quantum-optimization
    restart: unless-stopped
    ports:
      - "8006:8006"
    volumes:
      - ./src/services/quantum-optimization/logs:/app/logs
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - LOG_LEVEL=INFO
    networks:
      - rrr-backend
    healthcheck:
      test: ["CMD", "python", "-c", "from quantum.portfolio.qaoa_optimizer import QAOAOptimizer; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  monitoring:
    build:
      context: ./src/services/monitoring
      dockerfile: Dockerfile
    container_name: rrr-monitoring
    restart: unless-stopped
    ports:
      - "8501:8501"
      - "5001:5001"  # Health check API
    volumes:
      - ./src/services/monitoring/logs:/app/logs
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - LOG_LEVEL=INFO
      - DATABASE_POOL_MIN=5
      - DATABASE_POOL_MAX=10
      - PROMETHEUS_PORT=${PROMETHEUS_PORT:-9090}
    depends_on:
      redis:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    networks:
      - rrr-backend
      - rrr-frontend
      - rrr-monitoring
    healthcheck:
      test: ["CMD", "python", "-c", "from monitoring.performance_monitor import PerformanceMonitor; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # ==========================================================================
  # Supporting Services
  # ==========================================================================

  redis:
    image: redis:7-alpine
    container_name: rrr-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    networks:
      - rrr-backend
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  prometheus:
    image: prom/prometheus:latest
    container_name: rrr-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    networks:
      - rrr-backend
      - rrr-monitoring
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  grafana:
    image: grafana/grafana:latest
    container_name: rrr-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - rrr-monitoring
      - rrr-frontend
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

# ==============================================================================
# Networks - Simplified from 6 to 3 networks
# ==============================================================================

networks:
  rrr-backend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
    driver_opts:
      com.docker.network.bridge.name: rrr_backend

  rrr-frontend:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: rrr_frontend

  rrr-monitoring:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: rrr_monitoring

# ==============================================================================
# Volumes
# ==============================================================================

volumes:
  redis-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

# ==============================================================================
# Usage Instructions
# ==============================================================================
#
# Start all services:
#   docker-compose up -d
#
# Start specific service:
#   docker-compose up -d neural-network
#
# View logs:
#   docker-compose logs -f neural-network
#
# Stop all services:
#   docker-compose down
#
# Rebuild and restart:
#   docker-compose up -d --build
#
# Check service health:
#   docker-compose ps
#
# Access dashboards:
#   - Streamlit Dashboard: http://localhost:8501
#   - Grafana: http://localhost:3000
#   - Prometheus: http://localhost:9090
#
# ==============================================================================
